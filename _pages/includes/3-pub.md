# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2025 (Under Review)</div><img src='images/gsit.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs]()

**Yijie Jin**, Junjie Peng, Xuanchao Lin, Haochen Yuan, Lan Wang, Cangzhi Zheng

[**Project**](https://github.com/drewjin/GsiT) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- **Efficiency and Effectiveness**: 1/3 number of parameters without additional computational overhead compared with traditional MulTs.
- Block-Sparse Attention (Triton Kernel) and All-Modal-In-One Fusion.
</div>
</div>

<!-- - [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020** -->